{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "placed-white",
   "metadata": {},
   "source": [
    "# Homework Assignment 3\n",
    "\n",
    "Bradley Freed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rubber-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "\n",
    "from  sklearn.preprocessing  import  MinMaxScaler\n",
    "from  sklearn.model_selection  import  train_test_split\n",
    "from  sklearn.metrics  import  mean_squared_error\n",
    "from  sklearn.metrics  import  r2_score\n",
    "from  sklearn.metrics  import  accuracy_score\n",
    "\n",
    "\n",
    "filename = r\"C:\\Users\\Brad\\Downloads\\AMZN.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "data_select = data[['Adj Close']]\n",
    "values= data_select.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-compression",
   "metadata": {},
   "source": [
    "## Problem 1(a\n",
    "\n",
    "Transform the closing price data into time series data for 10 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  series_to_supervised(data , n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list  else  data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols , names = list(), list()\n",
    "    # input  sequence (t-n, ... t-1)\n",
    "    for i in  range(n_in , 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)'% (j+1, i)) for j in  range(n_vars)]\n",
    "    # forecast  sequence (t, t+1, ... t+n)\n",
    "    for i in  range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)'% (j+1)) for j in  range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)'% (j+1, i)) for j in  range(n_vars)]\n",
    "    # put it all  together\n",
    "    agg = pd.concat(cols , axis=1)\n",
    "    agg.columns = names\n",
    "    # drop  rows  with  NaN  values\n",
    "    if  dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return  agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "novel-fields",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       var1(t-10)    var1(t-9)    var1(t-8)    var1(t-7)    var1(t-6)  \\\n",
      "10       1.958333     1.729167     1.708333     1.635417     1.427083   \n",
      "11       1.729167     1.708333     1.635417     1.427083     1.395833   \n",
      "12       1.708333     1.635417     1.427083     1.395833     1.500000   \n",
      "13       1.635417     1.427083     1.395833     1.500000     1.583333   \n",
      "14       1.427083     1.395833     1.500000     1.583333     1.531250   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "5753  1676.609985  1785.000000  1689.150024  1807.839966  1830.000000   \n",
      "5754  1785.000000  1689.150024  1807.839966  1830.000000  1880.930054   \n",
      "5755  1689.150024  1807.839966  1830.000000  1880.930054  1846.089966   \n",
      "5756  1807.839966  1830.000000  1880.930054  1846.089966  1902.829956   \n",
      "5757  1830.000000  1880.930054  1846.089966  1902.829956  1940.099976   \n",
      "\n",
      "        var1(t-5)    var1(t-4)    var1(t-3)    var1(t-2)    var1(t-1)  \\\n",
      "10       1.395833     1.500000     1.583333     1.531250     1.505208   \n",
      "11       1.500000     1.583333     1.531250     1.505208     1.500000   \n",
      "12       1.583333     1.531250     1.505208     1.500000     1.510417   \n",
      "13       1.531250     1.505208     1.500000     1.510417     1.479167   \n",
      "14       1.505208     1.500000     1.510417     1.479167     1.416667   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "5753  1880.930054  1846.089966  1902.829956  1940.099976  1885.839966   \n",
      "5754  1846.089966  1902.829956  1940.099976  1885.839966  1955.489990   \n",
      "5755  1902.829956  1940.099976  1885.839966  1955.489990  1900.099976   \n",
      "5756  1940.099976  1885.839966  1955.489990  1900.099976  1963.949951   \n",
      "5757  1885.839966  1955.489990  1900.099976  1963.949951  1949.719971   \n",
      "\n",
      "          var1(t)  \n",
      "10       1.500000  \n",
      "11       1.510417  \n",
      "12       1.479167  \n",
      "13       1.416667  \n",
      "14       1.541667  \n",
      "...           ...  \n",
      "5753  1955.489990  \n",
      "5754  1900.099976  \n",
      "5755  1963.949951  \n",
      "5756  1949.719971  \n",
      "5757  1907.699951  \n",
      "\n",
      "[5748 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Send Adj Close values to series_to_supervised(). Returns time series data.\n",
    "array = series_to_supervised(values,10)\n",
    "\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-colorado",
   "metadata": {},
   "source": [
    "# Problem 1(b)\n",
    "Scale the data with MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yellow-taxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.59357128e-04, 1.53693076e-04, 1.44087293e-04, ...,\n",
       "        5.28325334e-05, 4.08251620e-05, 3.84236657e-05],\n",
       "       [1.53693076e-04, 1.44087293e-04, 1.10466888e-04, ...,\n",
       "        4.08251620e-05, 3.84236657e-05, 4.32266033e-05],\n",
       "       [1.44087293e-04, 1.10466888e-04, 1.44087293e-05, ...,\n",
       "        3.84236657e-05, 4.32266033e-05, 2.88177355e-05],\n",
       "       ...,\n",
       "       [7.78188587e-01, 8.32914067e-01, 8.43131601e-01, ...,\n",
       "        9.00991491e-01, 8.75452055e-01, 9.04892242e-01],\n",
       "       [8.32914067e-01, 8.43131601e-01, 8.66614396e-01, ...,\n",
       "        8.75452055e-01, 9.04892242e-01, 8.98331029e-01],\n",
       "       [8.43131601e-01, 8.66614396e-01, 8.50550352e-01, ...,\n",
       "        9.04892242e-01, 8.98331029e-01, 8.78956280e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MMS = MinMaxScaler()\n",
    "arrT = MMS.fit_transform(array)\n",
    "\n",
    "arrT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-bachelor",
   "metadata": {},
   "source": [
    "# Problem 1(c)\n",
    "Use the Normal Equation Method to find the linear regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prescription-movement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are :\n",
      " [[ 0.00012611]\n",
      " [-0.04833482]\n",
      " [ 0.22455881]\n",
      " [-0.15222874]\n",
      " [ 0.03289011]\n",
      " [-0.108812  ]\n",
      " [ 0.07175286]\n",
      " [-0.02607602]\n",
      " [-0.00148358]\n",
      " [ 0.08012983]\n",
      " [ 0.92804737]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = arrT[:,0:10]\n",
    "Y = arrT[:,10:11]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1)\n",
    "\n",
    "#add a constant 1 to X_train and X_test. Put it at the beginning. Î²0 will be at the beginning.\n",
    "\n",
    "ones = np.ones((len(X_train),1))\n",
    "X_train = np.append(ones,X_train,axis=1)\n",
    "\n",
    "ones = np.ones((len(X_test),1))\n",
    "X_test = np.append(ones,X_test,axis=1)\n",
    "\n",
    "X_transpose = np.transpose(X_train)\n",
    "\n",
    "# Calculating theta\n",
    "# Normal Equation:\n",
    "# theta = inv(X^T * X) * X^T * y\n",
    "theta = np.linalg.inv(X_transpose.dot(X_train))\n",
    "theta = theta.dot(X_transpose)\n",
    "theta = theta.dot(y_train)\n",
    "\n",
    "np.set_printoptions(precision=8,suppress=True)\n",
    "\n",
    "print('The coefficients are :\\n',theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-italic",
   "metadata": {},
   "source": [
    "## Problem 1(d)\n",
    "Make a prediction on my test set using the theta I found in the previous problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "popular-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  2.4148595608780825e-05\n",
      "Rsq:  0.9995593086029824\n"
     ]
    }
   ],
   "source": [
    "wT = np.transpose(theta)\n",
    "\n",
    "y_pred_norm = X_test.dot(theta)\n",
    "\n",
    "y_pred_norm\n",
    "\n",
    "print('MSE: ',mean_squared_error(y_test,y_pred_norm))\n",
    "\n",
    "print(\"Rsq: \",r2_score(y_test,y_pred_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-occasions",
   "metadata": {},
   "source": [
    "## Problem 1(e)\n",
    "Next,  find  the  coefficients using  gradient  descent  algorithm. Keep track of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "determined-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, coefficients):\n",
    "  yhat = coefficients[0]\n",
    "  for i in range(len(row)):\n",
    "    yhat = yhat + coefficients[i + 1] * row[i]\n",
    "  return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hungry-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficients_sgd(X_train, Y_train, l_rate, n_epoch):\n",
    "  #initializing all coefficients to zero\n",
    "  coef = [0.0 for i in range(len(X_train[0])+1)]\n",
    "  for epoch in range(n_epoch):\n",
    "    sum_error = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "      # calculating the prediction using current coeeficients\n",
    "      yhat = predict(X_train[i,:], coef)\n",
    "      # calculating error\n",
    "      error = yhat - Y_train[i]\n",
    "      sum_error += error**2\n",
    "      #stochastic gradient descent\n",
    "      coef[0] = coef[0] - l_rate * error\n",
    "      for j in range(len(coef)-1):\n",
    "        coef[j + 1] = coef[j + 1] - l_rate * error * X_train[i,j]\n",
    "    print( ' >epoch=%d, lrate=%.3f, error=%.3f ' % (epoch, l_rate, sum_error))\n",
    "  #returning the list of coefficients  \n",
    "\n",
    "  return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vietnamese-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >epoch=0, lrate=0.010, error=5.666 \n",
      " >epoch=1, lrate=0.010, error=0.453 \n",
      " >epoch=2, lrate=0.010, error=0.447 \n",
      " >epoch=3, lrate=0.010, error=0.442 \n",
      " >epoch=4, lrate=0.010, error=0.437 \n",
      " >epoch=5, lrate=0.010, error=0.432 \n",
      " >epoch=6, lrate=0.010, error=0.427 \n",
      " >epoch=7, lrate=0.010, error=0.422 \n",
      " >epoch=8, lrate=0.010, error=0.417 \n",
      " >epoch=9, lrate=0.010, error=0.413 \n",
      " >epoch=10, lrate=0.010, error=0.408 \n",
      " >epoch=11, lrate=0.010, error=0.404 \n",
      " >epoch=12, lrate=0.010, error=0.399 \n",
      " >epoch=13, lrate=0.010, error=0.395 \n",
      " >epoch=14, lrate=0.010, error=0.391 \n",
      " >epoch=15, lrate=0.010, error=0.387 \n",
      " >epoch=16, lrate=0.010, error=0.383 \n",
      " >epoch=17, lrate=0.010, error=0.379 \n",
      " >epoch=18, lrate=0.010, error=0.375 \n",
      " >epoch=19, lrate=0.010, error=0.371 \n",
      " >epoch=20, lrate=0.010, error=0.367 \n",
      " >epoch=21, lrate=0.010, error=0.364 \n",
      " >epoch=22, lrate=0.010, error=0.360 \n",
      " >epoch=23, lrate=0.010, error=0.357 \n",
      " >epoch=24, lrate=0.010, error=0.353 \n",
      " >epoch=25, lrate=0.010, error=0.350 \n",
      " >epoch=26, lrate=0.010, error=0.347 \n",
      " >epoch=27, lrate=0.010, error=0.343 \n",
      " >epoch=28, lrate=0.010, error=0.340 \n",
      " >epoch=29, lrate=0.010, error=0.337 \n",
      " >epoch=30, lrate=0.010, error=0.334 \n",
      " >epoch=31, lrate=0.010, error=0.331 \n",
      " >epoch=32, lrate=0.010, error=0.328 \n",
      " >epoch=33, lrate=0.010, error=0.326 \n",
      " >epoch=34, lrate=0.010, error=0.323 \n",
      " >epoch=35, lrate=0.010, error=0.320 \n",
      " >epoch=36, lrate=0.010, error=0.317 \n",
      " >epoch=37, lrate=0.010, error=0.315 \n",
      " >epoch=38, lrate=0.010, error=0.312 \n",
      " >epoch=39, lrate=0.010, error=0.310 \n",
      " >epoch=40, lrate=0.010, error=0.307 \n",
      " >epoch=41, lrate=0.010, error=0.305 \n",
      " >epoch=42, lrate=0.010, error=0.302 \n",
      " >epoch=43, lrate=0.010, error=0.300 \n",
      " >epoch=44, lrate=0.010, error=0.298 \n",
      " >epoch=45, lrate=0.010, error=0.296 \n",
      " >epoch=46, lrate=0.010, error=0.293 \n",
      " >epoch=47, lrate=0.010, error=0.291 \n",
      " >epoch=48, lrate=0.010, error=0.289 \n",
      " >epoch=49, lrate=0.010, error=0.287 \n",
      " >epoch=50, lrate=0.010, error=0.285 \n",
      " >epoch=51, lrate=0.010, error=0.283 \n",
      " >epoch=52, lrate=0.010, error=0.281 \n",
      " >epoch=53, lrate=0.010, error=0.279 \n",
      " >epoch=54, lrate=0.010, error=0.277 \n",
      " >epoch=55, lrate=0.010, error=0.275 \n",
      " >epoch=56, lrate=0.010, error=0.274 \n",
      " >epoch=57, lrate=0.010, error=0.272 \n",
      " >epoch=58, lrate=0.010, error=0.270 \n",
      " >epoch=59, lrate=0.010, error=0.268 \n",
      " >epoch=60, lrate=0.010, error=0.267 \n",
      " >epoch=61, lrate=0.010, error=0.265 \n",
      " >epoch=62, lrate=0.010, error=0.264 \n",
      " >epoch=63, lrate=0.010, error=0.262 \n",
      " >epoch=64, lrate=0.010, error=0.260 \n",
      " >epoch=65, lrate=0.010, error=0.259 \n",
      " >epoch=66, lrate=0.010, error=0.257 \n",
      " >epoch=67, lrate=0.010, error=0.256 \n",
      " >epoch=68, lrate=0.010, error=0.255 \n",
      " >epoch=69, lrate=0.010, error=0.253 \n",
      " >epoch=70, lrate=0.010, error=0.252 \n",
      " >epoch=71, lrate=0.010, error=0.250 \n",
      " >epoch=72, lrate=0.010, error=0.249 \n",
      " >epoch=73, lrate=0.010, error=0.248 \n",
      " >epoch=74, lrate=0.010, error=0.246 \n",
      " >epoch=75, lrate=0.010, error=0.245 \n",
      " >epoch=76, lrate=0.010, error=0.244 \n",
      " >epoch=77, lrate=0.010, error=0.243 \n",
      " >epoch=78, lrate=0.010, error=0.242 \n",
      " >epoch=79, lrate=0.010, error=0.240 \n",
      " >epoch=80, lrate=0.010, error=0.239 \n",
      " >epoch=81, lrate=0.010, error=0.238 \n",
      " >epoch=82, lrate=0.010, error=0.237 \n",
      " >epoch=83, lrate=0.010, error=0.236 \n",
      " >epoch=84, lrate=0.010, error=0.235 \n",
      " >epoch=85, lrate=0.010, error=0.234 \n",
      " >epoch=86, lrate=0.010, error=0.233 \n",
      " >epoch=87, lrate=0.010, error=0.232 \n",
      " >epoch=88, lrate=0.010, error=0.231 \n",
      " >epoch=89, lrate=0.010, error=0.230 \n",
      " >epoch=90, lrate=0.010, error=0.229 \n",
      " >epoch=91, lrate=0.010, error=0.228 \n",
      " >epoch=92, lrate=0.010, error=0.227 \n",
      " >epoch=93, lrate=0.010, error=0.226 \n",
      " >epoch=94, lrate=0.010, error=0.225 \n",
      " >epoch=95, lrate=0.010, error=0.224 \n",
      " >epoch=96, lrate=0.010, error=0.223 \n",
      " >epoch=97, lrate=0.010, error=0.223 \n",
      " >epoch=98, lrate=0.010, error=0.222 \n",
      " >epoch=99, lrate=0.010, error=0.221 \n",
      " >epoch=100, lrate=0.010, error=0.220 \n",
      " >epoch=101, lrate=0.010, error=0.219 \n",
      " >epoch=102, lrate=0.010, error=0.218 \n",
      " >epoch=103, lrate=0.010, error=0.218 \n",
      " >epoch=104, lrate=0.010, error=0.217 \n",
      " >epoch=105, lrate=0.010, error=0.216 \n",
      " >epoch=106, lrate=0.010, error=0.215 \n",
      " >epoch=107, lrate=0.010, error=0.215 \n",
      " >epoch=108, lrate=0.010, error=0.214 \n",
      " >epoch=109, lrate=0.010, error=0.213 \n",
      " >epoch=110, lrate=0.010, error=0.213 \n",
      " >epoch=111, lrate=0.010, error=0.212 \n",
      " >epoch=112, lrate=0.010, error=0.211 \n",
      " >epoch=113, lrate=0.010, error=0.211 \n",
      " >epoch=114, lrate=0.010, error=0.210 \n",
      " >epoch=115, lrate=0.010, error=0.209 \n",
      " >epoch=116, lrate=0.010, error=0.209 \n",
      " >epoch=117, lrate=0.010, error=0.208 \n",
      " >epoch=118, lrate=0.010, error=0.208 \n",
      " >epoch=119, lrate=0.010, error=0.207 \n",
      " >epoch=120, lrate=0.010, error=0.206 \n",
      " >epoch=121, lrate=0.010, error=0.206 \n",
      " >epoch=122, lrate=0.010, error=0.205 \n",
      " >epoch=123, lrate=0.010, error=0.205 \n",
      " >epoch=124, lrate=0.010, error=0.204 \n",
      " >epoch=125, lrate=0.010, error=0.204 \n",
      " >epoch=126, lrate=0.010, error=0.203 \n",
      " >epoch=127, lrate=0.010, error=0.203 \n",
      " >epoch=128, lrate=0.010, error=0.202 \n",
      " >epoch=129, lrate=0.010, error=0.201 \n",
      " >epoch=130, lrate=0.010, error=0.201 \n",
      " >epoch=131, lrate=0.010, error=0.200 \n",
      " >epoch=132, lrate=0.010, error=0.200 \n",
      " >epoch=133, lrate=0.010, error=0.200 \n",
      " >epoch=134, lrate=0.010, error=0.199 \n",
      " >epoch=135, lrate=0.010, error=0.199 \n",
      " >epoch=136, lrate=0.010, error=0.198 \n",
      " >epoch=137, lrate=0.010, error=0.198 \n",
      " >epoch=138, lrate=0.010, error=0.197 \n",
      " >epoch=139, lrate=0.010, error=0.197 \n",
      " >epoch=140, lrate=0.010, error=0.196 \n",
      " >epoch=141, lrate=0.010, error=0.196 \n",
      " >epoch=142, lrate=0.010, error=0.195 \n",
      " >epoch=143, lrate=0.010, error=0.195 \n",
      " >epoch=144, lrate=0.010, error=0.195 \n",
      " >epoch=145, lrate=0.010, error=0.194 \n",
      " >epoch=146, lrate=0.010, error=0.194 \n",
      " >epoch=147, lrate=0.010, error=0.193 \n",
      " >epoch=148, lrate=0.010, error=0.193 \n",
      " >epoch=149, lrate=0.010, error=0.193 \n",
      " >epoch=150, lrate=0.010, error=0.192 \n",
      " >epoch=151, lrate=0.010, error=0.192 \n",
      " >epoch=152, lrate=0.010, error=0.192 \n",
      " >epoch=153, lrate=0.010, error=0.191 \n",
      " >epoch=154, lrate=0.010, error=0.191 \n",
      " >epoch=155, lrate=0.010, error=0.190 \n",
      " >epoch=156, lrate=0.010, error=0.190 \n",
      " >epoch=157, lrate=0.010, error=0.190 \n",
      " >epoch=158, lrate=0.010, error=0.189 \n",
      " >epoch=159, lrate=0.010, error=0.189 \n",
      " >epoch=160, lrate=0.010, error=0.189 \n",
      " >epoch=161, lrate=0.010, error=0.188 \n",
      " >epoch=162, lrate=0.010, error=0.188 \n",
      " >epoch=163, lrate=0.010, error=0.188 \n",
      " >epoch=164, lrate=0.010, error=0.187 \n",
      " >epoch=165, lrate=0.010, error=0.187 \n",
      " >epoch=166, lrate=0.010, error=0.187 \n",
      " >epoch=167, lrate=0.010, error=0.187 \n",
      " >epoch=168, lrate=0.010, error=0.186 \n",
      " >epoch=169, lrate=0.010, error=0.186 \n",
      " >epoch=170, lrate=0.010, error=0.186 \n",
      " >epoch=171, lrate=0.010, error=0.185 \n",
      " >epoch=172, lrate=0.010, error=0.185 \n",
      " >epoch=173, lrate=0.010, error=0.185 \n",
      " >epoch=174, lrate=0.010, error=0.184 \n",
      " >epoch=175, lrate=0.010, error=0.184 \n",
      " >epoch=176, lrate=0.010, error=0.184 \n",
      " >epoch=177, lrate=0.010, error=0.184 \n",
      " >epoch=178, lrate=0.010, error=0.183 \n",
      " >epoch=179, lrate=0.010, error=0.183 \n",
      " >epoch=180, lrate=0.010, error=0.183 \n",
      " >epoch=181, lrate=0.010, error=0.183 \n",
      " >epoch=182, lrate=0.010, error=0.182 \n",
      " >epoch=183, lrate=0.010, error=0.182 \n",
      " >epoch=184, lrate=0.010, error=0.182 \n",
      " >epoch=185, lrate=0.010, error=0.182 \n",
      " >epoch=186, lrate=0.010, error=0.181 \n",
      " >epoch=187, lrate=0.010, error=0.181 \n",
      " >epoch=188, lrate=0.010, error=0.181 \n",
      " >epoch=189, lrate=0.010, error=0.181 \n",
      " >epoch=190, lrate=0.010, error=0.180 \n",
      " >epoch=191, lrate=0.010, error=0.180 \n",
      " >epoch=192, lrate=0.010, error=0.180 \n",
      " >epoch=193, lrate=0.010, error=0.180 \n",
      " >epoch=194, lrate=0.010, error=0.179 \n",
      " >epoch=195, lrate=0.010, error=0.179 \n",
      " >epoch=196, lrate=0.010, error=0.179 \n",
      " >epoch=197, lrate=0.010, error=0.179 \n",
      " >epoch=198, lrate=0.010, error=0.179 \n",
      " >epoch=199, lrate=0.010, error=0.178 \n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.01\n",
    "n_epoch = 200\n",
    "\n",
    "coef = coefficients_sgd(X_train[:,1:11], y_train, l_rate, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "killing-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00001577,  0.00571104,  0.00177837, -0.0268984 , -0.01280302,\n",
       "       -0.00269069,  0.04673011,  0.09593432,  0.1690996 ,  0.28114747,\n",
       "        0.43780402])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=8,suppress=True)\n",
    "\n",
    "np.concatenate(coef, axis=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-quest",
   "metadata": {},
   "source": [
    "## Problem 1(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "small-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  3.536825552792772e-05\n",
      "Rsq:  0.9993545593213291\n"
     ]
    }
   ],
   "source": [
    "y_pred_sgd = X_test.dot(coef)\n",
    "\n",
    "\n",
    "print('MSE: ',mean_squared_error(y_test,y_pred_sgd))\n",
    "\n",
    "print(\"Rsq: \",r2_score(y_test,y_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "included-astrology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal equation method has just barely better metrics than gradient descent.\n"
     ]
    }
   ],
   "source": [
    "print('Normal equation method has just barely better metrics than gradient descent.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-pavilion",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "anonymous-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from  sklearn.model_selection  import  train_test_split\n",
    "from  sklearn.model_selection  import  GridSearchCV\n",
    "from  sklearn.model_selection  import  RepeatedStratifiedKFold\n",
    "from  sklearn.linear_model  import  Perceptron\n",
    "from  sklearn.model_selection  import  cross_val_score\n",
    "\n",
    "\n",
    "filename = r\"C:\\Users\\Brad\\Downloads\\sonar.all-data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "array = df.values\n",
    "\n",
    "X = array[:,:-1]\n",
    "Y = array[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-science",
   "metadata": {},
   "source": [
    "## Problem 2(a)\n",
    "Split the data, define the model and set my RepeatedStratifiedKFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "protected-trance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.664 (0.119)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=3)\n",
    "\n",
    "model = Perceptron()\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=rskf, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-birth",
   "metadata": {},
   "source": [
    "## Problem 2(b)\n",
    "Set the alpha values for the grid. Then use GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "variable-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict()\n",
    "grid['alpha'] = [0.0001, 0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cordless-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=rskf, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-durham",
   "metadata": {},
   "source": [
    "## Problem 2(c)\n",
    "Report the best score and best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "polyphonic-mercy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.664\n",
      "Config: {'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-stability",
   "metadata": {},
   "source": [
    "## Problem 2(d)\n",
    "Create the perceptron model and fit using the training data. Make predictions from X_test to Y_test and look at the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stuffed-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6507936507936508\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(alpha=.0001)\n",
    "model.fit(X_train,y_train)\n",
    "print('Accuracy:',model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-attendance",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "chief-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.neighbors  import  KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = r\"C:\\Users\\Brad\\Downloads\\sonar.all-data.csv\"\n",
    "df3 = pd.read_csv(filename)\n",
    "\n",
    "array3 = df3.values\n",
    "\n",
    "X3 = array[:,:-1]\n",
    "Y3 = array[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-stamp",
   "metadata": {},
   "source": [
    "## Problem 3(a)\n",
    "Split the data and test accuracy of KNN with n nearest neighbors equaling from 1 to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lucky-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=5)\n",
    "\n",
    "scores = {}\n",
    "for k in range(1,30):\n",
    "  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  scores[k] = accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-leadership",
   "metadata": {},
   "source": [
    "## Problem 3(b)\n",
    "plot the accuracies and choose the best value for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "curious-naples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15cbca7d908>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(scores.keys()),list(scores.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "white-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best value for k appears to be 1.\n"
     ]
    }
   ],
   "source": [
    "print(\"the best value for k appears to be 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-resident",
   "metadata": {},
   "source": [
    "## Problem 3(c)\n",
    "Create the K nearest neighbor with the chosen value of k. Perform predictions on the test set and report accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "decent-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the model with n_neighbor = 1 is: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print('The accuracy score of the model with n_neighbor = 1 is:',accuracy_score(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
